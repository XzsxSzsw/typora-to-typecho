import os
import re
import time
import sys
import random
import shutil
import yaml
import requests
import urllib3
import brotli
from ftplib import FTP, error_perm, error_temp
from urllib.parse import urljoin, urlparse
from pathlib import Path


# ========== YAML é…ç½®è¯»å– + æ’å€¼è§£æï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰ ==========
def load_yaml_config(config_path="config.yaml"):
    """è¯»å–YAMLé…ç½®å¹¶è§£æå¤šå±‚åµŒå¥—æ’å€¼ï¼Œç¡®ä¿è¿”å›å­—å…¸"""
    # 1. æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼")
        sys.exit(1)

    # 2. è¯»å–åŸå§‹YAMLé…ç½®
    try:
        with open(config_path, encoding="utf-8") as f:
            raw_config = yaml.safe_load(f)
    except yaml.YAMLError as e:
        print(f"âŒ YAMLé…ç½®æ–‡ä»¶è§£æé”™è¯¯ï¼š{e}")
        sys.exit(1)

    # 3. æ ¡éªŒåŸå§‹é…ç½®ç±»å‹
    if not isinstance(raw_config, dict):
        print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼åº”ä¸ºå­—å…¸ç±»å‹ï¼Œå®é™…ï¼š{type(raw_config)}")
        sys.exit(1)
    print(f"âœ… åŸå§‹é…ç½®è¯»å–æˆåŠŸï¼Œç±»å‹ï¼š{type(raw_config)}")

    # 4. å®šä¹‰é€’å½’+å¾ªç¯è§£ææ’å€¼çš„å‡½æ•°ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šç¼©è¿›+é€»è¾‘å®Œæ•´ï¼‰
    def resolve_interpolations(item, config):
        """
        é€’å½’è§£ææ‰€æœ‰æ’å€¼ï¼Œæ”¯æŒå¤šå±‚åµŒå¥—
        :param item: å½“å‰è¦è§£æçš„å…ƒç´ ï¼ˆå­—ç¬¦ä¸²/å­—å…¸/åˆ—è¡¨ï¼‰
        :param config: å®Œæ•´é…ç½®å­—å…¸ï¼ˆç”¨äºæ’å€¼æŸ¥æ‰¾ï¼‰
        :return: è§£æåçš„å…ƒç´ 
        """
        # å¤„ç†å­—ç¬¦ä¸²ç±»å‹ï¼šå¾ªç¯è§£æç›´åˆ°æ— ${}å ä½ç¬¦
        if isinstance(item, str):
            current_str = item
            # å¾ªç¯è§£æï¼Œç¡®ä¿å¤šå±‚åµŒå¥—æ’å€¼è¢«å®Œå…¨æ›¿æ¢
            while "${" in current_str:
                # åŒ¹é…æ‰€æœ‰${xxx.xxx}æ ¼å¼çš„æ’å€¼
                pattern = r"\$\{([\w.]+)\}"
                matches = re.findall(pattern, current_str)
                if not matches:
                    break  # æ— æ’å€¼ï¼Œé€€å‡ºå¾ªç¯

                for match in matches:
                    # æ‹†åˆ†æ’å€¼è·¯å¾„ï¼ˆå¦‚ global.domain â†’ ["global", "domain"]ï¼‰
                    keys = match.split(".")
                    val = config
                    try:
                        # é€å±‚æŸ¥æ‰¾æ’å€¼å˜é‡
                        for key in keys:
                            val = val[key]
                        # æ›¿æ¢æ’å€¼ï¼ˆç¡®ä¿å€¼ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼‰
                        current_str = current_str.replace(f"${{{match}}}", str(val))
                    except KeyError:
                        print(f"âŒ æ’å€¼å˜é‡ '{match}' ä¸å­˜åœ¨äºé…ç½®ä¸­ï¼")
                        sys.exit(1)
            return current_str

        # å¤„ç†å­—å…¸ç±»å‹ï¼šé€’å½’è§£ææ¯ä¸ªé”®å€¼å¯¹
        elif isinstance(item, dict):
            new_dict = {}
            for k, v in item.items():
                new_dict[k] = resolve_interpolations(v, config)
            return new_dict

        # å¤„ç†åˆ—è¡¨ç±»å‹ï¼šé€’å½’è§£ææ¯ä¸ªå…ƒç´ 
        elif isinstance(item, list):
            new_list = []
            for elem in item:
                new_list.append(resolve_interpolations(elem, config))
            return new_list

        # å…¶ä»–ç±»å‹ï¼ˆæ•°å­—/å¸ƒå°”/Noneï¼‰ç›´æ¥è¿”å›
        else:
            return item

    # 5. è§£ææ‰€æœ‰é…ç½®
    resolved_config = resolve_interpolations(raw_config, raw_config)

    # 6. æœ€ç»ˆæ ¡éªŒ+è°ƒè¯•æ‰“å°
    if not isinstance(resolved_config, dict):
        print(f"âŒ æ’å€¼è§£æåé…ç½®ç±»å‹é”™è¯¯ï¼åº”ä¸ºå­—å…¸ï¼Œå®é™…ï¼š{type(resolved_config)}")
        sys.exit(1)

    # è°ƒè¯•æ‰“å°å…³é”®æ’å€¼ç»“æœ
    print("\nğŸ“Œ æ’å€¼ç»“æœéªŒè¯ï¼š")
    print(f"   - ç«™ç‚¹åŸŸåï¼š{resolved_config['global']['domain']}")
    print(f"   - é¦–é¡µURLï¼š{resolved_config['site']['home_url']}")
    print(f"   - ç™»å½•é¡µURLï¼š{resolved_config['site']['login_page']}")
    print(f"   - å›¾ç‰‡æœåŠ¡å™¨URLï¼š{resolved_config['image']['server_img_url']}")

    return resolved_config


# åŠ è½½å¹¶è§£æé…ç½®
config = load_yaml_config()

# ========== é…ç½®å‚æ•°æå–ï¼ˆä¸åŸé€»è¾‘ä¸€è‡´ï¼‰ ==========
# ç«™ç‚¹é…ç½®
SITE_DOMAIN = config["global"]["domain"]
SITE_HOME = config["site"]["home_url"]
TYPECHO_LOGIN_PAGE = config["site"]["login_page"]
TYPECHO_ADMIN_URL = config["site"]["admin_url"]
TYPECHO_WRITE_URL = config["site"]["write_post_url"]
TYPECHO_MANAGE_POSTS_URL = config["site"]["manage_posts_url"]
TYPECHO_MANAGE_CATEGORIES_URL = config["site"]["manage_categories_url"]
TYPECHO_DELETE_POST_URL = config["site"]["delete_post_url"]
TIMEZONE = config["global"]["timezone"]

# ç™»å½•é…ç½®
USERNAME = config["login"]["username"]
PASSWORD = config["login"]["password"]
COOKIE_PREFIX = config["login"]["cookie_prefix"]

# å›¾ç‰‡é…ç½®
IMG_ROOT_DIR = config["image"]["processed_img_root"]
IMG_SERVER_URL = config["image"]["server_img_url"]
SPACE_REPLACE_CHAR = config["image"]["space_replace_char"]

# FTPé…ç½®
FTP_HOST = config["ftp"]["host"]
FTP_PORT = config["ftp"]["port"]
FTP_USER = config["ftp"]["user"]
FTP_PWD = config["ftp"]["password"]
FTP_IMG_BASE_PATH = config["ftp"]["base_path"]
FTP_TIMEOUT = config["ftp"]["timeout"]
FTP_PASSIVE = config["ftp"]["passive"]

# è¯·æ±‚é…ç½®
USER_AGENT = config["request"]["user_agent"]
MIN_DELAY = config["request"]["min_delay"]
MAX_DELAY = config["request"]["max_delay"]
BATCH_DELAY = config["request"]["batch_delay"]

# åˆ†ç±»é…ç½®
DEFAULT_CATEGORY_ID = config["category"]["default_category_id"]

# ========== ä»¥ä¸‹ä»£ç å®Œå…¨ä¿ç•™ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰ ==========
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

CHROME_HEADERS = {
    "User-Agent": USER_AGENT,
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Ch-Ua": '"Chromium";v="128", "Not;A=Brand";v="24", "Google Chrome";v="128"',
    "Sec-Ch-Ua-Mobile": "?0",
    "Sec-Ch-Ua-Platform": '"Windows"',
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
    "Priority": "u=0, i",
    "Cache-Control": "max-age=0",
}

ADMIN_KEYWORDS = ["ç½‘ç«™æ¦‚è¦", "ç®¡ç†é¢æ¿", "æ–‡ç« ç®¡ç†", "é€€å‡ºç™»å½•", "Typecho"]

# å…¨å±€å˜é‡
img_mapping = {}
article_id = None
ftp_conn = None
note_img_local_dir = ""
temp_content = ""
category_map = {}
batch_stats = {"total": 0, "success": 0, "failed": 0, "failed_files": []}


# ========== æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆå®Œå…¨ä¿ç•™ï¼‰ ==========
def decode_response(resp: requests.Response) -> str:
    try:
        resp.encoding = "utf-8"
        ce = resp.headers.get("Content-Encoding", "").lower()
        if ce == "br":
            return brotli.decompress(resp.content).decode("utf-8", errors="ignore")
        elif ce in ["gzip", "deflate"]:
            return resp.text
        return resp.content.decode("utf-8", errors="ignore")
    except:
        return resp.content.decode("utf-8", errors="ignore")


def print_step(title: str, step_num: int):
    print("\n" + "=" * 80)
    print(f"ğŸ“Œ æ‰§è¡Œæ­¥éª¤ [{step_num}/7]ï¼š{title}")
    print("=" * 80)


def human_delay(min_seconds: float = None, max_seconds: float = None):
    min_s = min_seconds if min_seconds else MIN_DELAY
    max_s = max_seconds if max_seconds else MAX_DELAY
    delay = random.uniform(min_s, max_s)
    print(f"â³ æ¨¡æ‹Ÿäººç±»æ“ä½œå»¶è¿Ÿï¼š{delay:.2f}ç§’")
    time.sleep(delay)


def check_cookie(session: requests.Session, cookie_name: str) -> tuple[bool, str]:
    cookie_value = session.cookies.get(cookie_name, domain=SITE_DOMAIN)
    if not cookie_value:
        cookie_value = session.cookies.get(cookie_name, domain=f".{SITE_DOMAIN}")
    return (True, cookie_value) if cookie_value else (False, "")


def check_admin_keyword(admin_html: str) -> tuple[bool, str]:
    if "ç½‘ç«™æ¦‚è¦" in admin_html:
        return True, "ç½‘ç«™æ¦‚è¦"
    for keyword in ADMIN_KEYWORDS[1:]:
        if keyword in admin_html:
            return True, keyword
    return False, ""


def update_referer_headers(session: requests.Session, referer_url: str):
    session.headers["Referer"] = referer_url
    if referer_url == "":
        session.headers["Sec-Fetch-Site"] = "none"
    elif urlparse(referer_url).netloc == SITE_DOMAIN:
        session.headers["Sec-Fetch-Site"] = "same-origin"
    else:
        session.headers["Sec-Fetch-Site"] = "cross-site"


def clean_local_img_dir(dir_path: str):
    if os.path.exists(dir_path):
        try:
            shutil.rmtree(dir_path)
            print(f"âœ… å·²æ¸…ç†æœ¬åœ°å›¾ç‰‡ç›®å½•ï¼š{dir_path}")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æœ¬åœ°ç›®å½•å¤±è´¥ï¼š{e}")


def reset_global_vars():
    global img_mapping, article_id, note_img_local_dir, temp_content
    img_mapping = {}
    article_id = None
    note_img_local_dir = ""
    temp_content = ""


def replace_space_char(text: str) -> str:
    return text.replace(" ", SPACE_REPLACE_CHAR).strip()


def parse_user_selection(input_str: str, max_num: int) -> list[int]:
    selected = []
    parts = input_str.strip().split()
    for part in parts:
        if "-" in part:
            try:
                start, end = part.split("-")
                start = int(start)
                end = int(end)
                if 1 <= start <= end <= max_num:
                    selected.extend(range(start, end + 1))
                else:
                    print(f"âš ï¸ æ— æ•ˆèŒƒå›´ï¼š{part}ï¼Œè¯·ç¡®ä¿åœ¨1-{max_num}ä¹‹é—´")
            except ValueError:
                print(f"âš ï¸ æ— æ•ˆè¾“å…¥ï¼š{part}ï¼Œè¯·è¾“å…¥æ•°å­—æˆ–èŒƒå›´å¦‚1-3")
        else:
            try:
                num = int(part)
                if 1 <= num <= max_num:
                    selected.append(num)
                else:
                    print(f"âš ï¸ æ— æ•ˆç¼–å·ï¼š{num}ï¼Œè¯·ç¡®ä¿åœ¨1-{max_num}ä¹‹é—´")
            except ValueError:
                print(f"âš ï¸ æ— æ•ˆè¾“å…¥ï¼š{part}ï¼Œè¯·è¾“å…¥æ•°å­—")
    return sorted(list(set(selected)))


def rollback_article(session: requests.Session, article_id: str):
    if not article_id:
        return
    print(f"\nğŸ”„ å¼€å§‹å›æ»šï¼šåˆ é™¤æ–‡ç« ID {article_id}")
    try:
        edit_url = f"{SITE_HOME}/admin/write-post.php?cid={article_id}"
        update_referer_headers(session, TYPECHO_MANAGE_POSTS_URL)
        edit_resp = session.get(edit_url, timeout=10)
        edit_html = decode_response(edit_resp)

        publish_token = re.search(r"_=([0-9a-f]{32})", edit_html)
        publish_token = (
            publish_token.group(1)
            if publish_token
            else "4bc337a9bb2079e48605260b98bcc6d8"
        )
        csrf_token = re.search(r'name="__typecho_csrf_token" value="(.*?)"', edit_html)
        csrf_token = csrf_token.group(1) if csrf_token else ""

        delete_api = (
            f"{SITE_HOME}/index.php/action/contents-post-edit?_={publish_token}"
        )
        delete_data = {
            "cid": article_id,
            "do": "delete",
            "__typecho_csrf_token": csrf_token,
        }
        delete_headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Referer": edit_url,
            **CHROME_HEADERS,
        }
        delete_resp = session.post(
            delete_api, data=delete_data, headers=delete_headers, timeout=10
        )
        if delete_resp.status_code in [200, 302]:
            print(f"âœ… å›æ»šæˆåŠŸï¼šå·²åˆ é™¤æ–‡ç« ID {article_id}")
        else:
            print(f"âŒ å›æ»šå¤±è´¥ï¼šåˆ é™¤æ–‡ç« ID {article_id} å¤±è´¥")
    except Exception as e:
        print(f"âŒ å›æ»šå¼‚å¸¸ï¼š{e}")


def rollback_ftp_files(remote_dir: str):
    if not ftp_conn or not remote_dir:
        return
    print(f"\nğŸ”„ å¼€å§‹å›æ»šï¼šåˆ é™¤FTPç›®å½• {remote_dir} ä¸‹çš„å›¾ç‰‡")
    try:
        original_dir = ftp_conn.pwd()
        ftp_conn.cwd(remote_dir)
        file_list = ftp_conn.nlst()
        for file in file_list:
            ftp_conn.delete(file)
            print(f"âœ… åˆ é™¤FTPæ–‡ä»¶ï¼š{file}")
        ftp_conn.cwd(original_dir)
        ftp_conn.rmd(remote_dir)
        print(f"âœ… åˆ é™¤FTPç©ºç›®å½•ï¼š{remote_dir}")
    except Exception as e:
        print(f"âŒ FTPå›æ»šå¤±è´¥ï¼š{e}")


# ========== 1. ç™»å½•éªŒè¯ï¼ˆå®Œå…¨ä¿ç•™åŸé€»è¾‘ï¼‰ ==========
def simulate_browser_login() -> tuple[bool, requests.Session]:
    print_step("ç™»å½•éªŒè¯", 1)
    session = requests.Session()
    session.headers.update(CHROME_HEADERS)
    session.headers["Referer"] = ""
    session.adapters.DEFAULT_POOLSIZE = 1
    session.verify = False
    session.timeout = 60

    print("âœ… Sessionåˆå§‹åŒ–å®Œæˆ")
    human_delay()

    try:
        home_resp = session.get(SITE_HOME, allow_redirects=True, timeout=10)
        print(f"ğŸ“ˆ é¦–é¡µå“åº”ç ï¼š{home_resp.status_code}")
    except Exception as e:
        print(f"âš ï¸ é¦–é¡µè®¿é—®è­¦å‘Šï¼š{e}")
    human_delay(1.5, 2.5)

    update_referer_headers(session, SITE_HOME)
    try:
        login_resp = session.get(TYPECHO_LOGIN_PAGE, allow_redirects=True, timeout=10)
        login_html = decode_response(login_resp)

        action_patterns = [
            r'<form.*?action="(.*?index.php/action/login\?_=.*?)".*?>',
            r'action="(.*?/action/login\?_=.*?)"',
            r'<form[^>]*?action="([^"]+)"',
        ]
        real_login_url = None
        for pattern in action_patterns:
            match = re.search(pattern, login_html, re.DOTALL | re.IGNORECASE)
            if match:
                real_login_url = match.group(1)
                break

        if not real_login_url:
            print("âŒ æœªæ‰¾åˆ°ç™»å½•æ¥å£")
            return False, session
        if not real_login_url.startswith("http"):
            real_login_url = urljoin(TYPECHO_LOGIN_PAGE, real_login_url)
        print(f"âœ… æ‰¾åˆ°ç™»å½•æ¥å£ï¼š{real_login_url}")
    except Exception as e:
        print(f"âŒ ç™»å½•é¡µè®¿é—®å¤±è´¥ï¼š{e}")
        return False, session
    human_delay(2.0, 3.0)

    update_referer_headers(session, TYPECHO_LOGIN_PAGE)
    login_data = {
        "name": USERNAME,
        "password": PASSWORD,
        "referer": TYPECHO_ADMIN_URL,
        "login": "ç™»å½•",
    }

    try:
        login_resp = session.post(
            real_login_url,
            data=login_data,
            allow_redirects=True,
            timeout=10,
            headers={"Content-Type": "application/x-www-form-urlencoded"},
        )

        auth_exists = check_cookie(session, f"{COOKIE_PREFIX}__typecho_authCode")[0]
        uid_exists = check_cookie(session, f"{COOKIE_PREFIX}__typecho_uid")[0]
        php_exists = check_cookie(session, "PHPSESSID")[0]

        print(f"\nğŸ“Œ ç™»å½•éªŒè¯ç»“æœï¼š")
        print(f"   AuthCodeï¼š{'âœ… å­˜åœ¨' if auth_exists else 'âŒ ç¼ºå¤±'}")
        print(f"   UIDï¼š{'âœ… å­˜åœ¨' if uid_exists else 'âŒ ç¼ºå¤±'}")
        print(f"   PHPSESSIDï¼š{'âœ… å­˜åœ¨' if php_exists else 'âŒ ç¼ºå¤±'}")

        if not (auth_exists and uid_exists and php_exists):
            print("âŒ ç™»å½•å¤±è´¥ï¼šæ ¸å¿ƒCookieç¼ºå¤±")
            return False, session
    except Exception as e:
        print(f"âŒ ç™»å½•æäº¤å¤±è´¥ï¼š{e}")
        return False, session
    human_delay(1.0, 2.0)

    update_referer_headers(session, TYPECHO_LOGIN_PAGE)
    try:
        admin_resp = session.get(TYPECHO_ADMIN_URL, allow_redirects=False, timeout=10)
        admin_html = decode_response(admin_resp)

        if check_admin_keyword(admin_html)[0]:
            print("âœ… åå°è®¿é—®æˆåŠŸï¼Œç™»å½•éªŒè¯é€šè¿‡")
            return True, session
        else:
            print("âŒ åå°éªŒè¯å¤±è´¥ï¼šæœªæ‰¾åˆ°å…³é”®æ ‡è¯†")
            return False, session
    except Exception as e:
        print(f"âŒ åå°è®¿é—®å¤±è´¥ï¼š{e}")
        return False, session


# ========== 2. æŠ“å–æ–‡ç« åˆ†ç±»ï¼ˆå®Œå…¨ä¿ç•™ï¼‰ ==========
def crawl_categories(session: requests.Session) -> bool:
    print_step("æŠ“å–æ–‡ç« åˆ†ç±»", 2)
    global category_map
    category_map.clear()

    try:
        update_referer_headers(session, TYPECHO_ADMIN_URL)
        cate_resp = session.get(TYPECHO_MANAGE_CATEGORIES_URL, timeout=10)
        if cate_resp.status_code != 200:
            print(f"âŒ åˆ†ç±»é¡µè®¿é—®å¤±è´¥ï¼šçŠ¶æ€ç  {cate_resp.status_code}")
            category_map[DEFAULT_CATEGORY_ID] = "é»˜è®¤åˆ†ç±»"
            print(f"âš ï¸ ä½¿ç”¨é»˜è®¤åˆ†ç±»ï¼š[{DEFAULT_CATEGORY_ID}] é»˜è®¤åˆ†ç±»")
            return True
        cate_html = decode_response(cate_resp)

        cate_pattern = r'<a href="[^"]*category\.php\?mid=(\d+)"[^>]*>([^<]+)</a>'
        matches = re.findall(cate_pattern, cate_html)
        if not matches:
            print("âŒ æœªæŠ“å–åˆ°ä»»ä½•åˆ†ç±»ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»")
            category_map[DEFAULT_CATEGORY_ID] = "é»˜è®¤åˆ†ç±»"
            return True

        for mid, cate_name in matches:
            mid = int(mid)
            cate_name = cate_name.strip()
            category_map[mid] = cate_name

        print(f"âœ… æˆåŠŸæŠ“å– {len(category_map)} ä¸ªåˆ†ç±»ï¼š")
        for idx, (mid, name) in enumerate(category_map.items(), 1):
            print(f"   {idx}. [{mid}] {name}")
        return True
    except Exception as e:
        print(f"âŒ æŠ“å–åˆ†ç±»å¤±è´¥ï¼š{e}")
        category_map[DEFAULT_CATEGORY_ID] = "é»˜è®¤åˆ†ç±»"
        print(f"âš ï¸ ä½¿ç”¨é»˜è®¤åˆ†ç±»ï¼š[{DEFAULT_CATEGORY_ID}] é»˜è®¤åˆ†ç±»")
        return True


def select_categories() -> list[int]:
    if not category_map:
        print(f"âŒ æ— å¯ç”¨åˆ†ç±»ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»ID {DEFAULT_CATEGORY_ID}")
        return [DEFAULT_CATEGORY_ID]

    print("\n" + "=" * 50)
    print("ğŸ“‹ å¯é€‰åˆ†ç±»åˆ—è¡¨ï¼ˆåŸºäºåå°æŠ“å–ï¼‰")
    print("=" * 50)
    sorted_cates = sorted(category_map.items(), key=lambda x: x[0])
    cate_idx_map = {}
    for idx, (mid, name) in enumerate(sorted_cates, 1):
        cate_idx_map[idx] = mid
        print(f"   {idx}. [{mid}] {name}")
    print("=" * 50)
    print("æç¤ºï¼šè¾“å…¥åˆ†ç±»ç¼–å·ï¼Œæ”¯æŒå•ä¸ª(å¦‚1)ã€å¤šä¸ª(å¦‚1 3)ã€èŒƒå›´(å¦‚1-2)")
    print("ç›´æ¥å›è½¦ä½¿ç”¨ç¬¬ä¸€ä¸ªåˆ†ç±»")

    while True:
        user_input = input("è¯·è¾“å…¥åˆ†ç±»é€‰æ‹©ï¼š").strip()
        if not user_input:
            default_mid = sorted_cates[0][0]
            print(f"âœ… é€‰æ‹©é»˜è®¤åˆ†ç±»ï¼š[{default_mid}] {category_map[default_mid]}")
            return [default_mid]

        selected_idxs = parse_user_selection(user_input, len(sorted_cates))
        if selected_idxs:
            selected_mids = [cate_idx_map[idx] for idx in selected_idxs]
            print(f"âœ… å·²é€‰æ‹©åˆ†ç±»ï¼š")
            for mid in selected_mids:
                print(f"   - [{mid}] {category_map[mid]}")
            return selected_mids
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥ï¼")


# ========== 3. Markdownæ ¼å¼æ¸…æ´— + åŠ¨æ€å›¾ç‰‡è·¯å¾„å¤„ç†ï¼ˆå®Œå…¨ä¿ç•™ï¼‰ ==========
def process_markdown_images(
    raw_content: str, md_file_path: str
) -> tuple[str, dict, bool]:
    global img_mapping, note_img_local_dir, temp_content
    img_mapping = {}
    content = raw_content
    img_counter = 1
    process_success = True

    md_basename = os.path.splitext(os.path.basename(md_file_path))[0]
    md_basename = replace_space_char(md_basename)
    md_dir = os.path.dirname(md_file_path)

    note_img_local_dir = os.path.join(IMG_ROOT_DIR, md_basename)

    try:
        Path(note_img_local_dir).mkdir(parents=True, exist_ok=True)
        print(f"âœ… åˆ›å»ºå¤„ç†åå›¾ç‰‡ç›®å½•ï¼š{note_img_local_dir}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºæœ¬åœ°ç›®å½•å¤±è´¥ï¼š{e}")
        return content, img_mapping, False

    img_pattern = r"!\[(.*?)\]\((.*?\.(png|jpg|jpeg|gif|webp))\)"
    img_pattern = re.compile(img_pattern, re.IGNORECASE | re.DOTALL)

    def replace_img_path(match):
        nonlocal img_counter, process_success
        alt_text = match.group(1)
        img_path = match.group(2).strip()
        img_ext = match.group(3).lower()

        if os.path.isabs(img_path):
            raw_img_path = img_path
        else:
            raw_img_path = os.path.join(md_dir, img_path)
        raw_img_path = os.path.abspath(raw_img_path)
        raw_img_path = raw_img_path.replace("/", "\\")

        new_filename = f"{md_basename}_{int(time.time())}_{img_counter}.{img_ext}"
        new_filename = replace_space_char(new_filename)
        img_counter += 1
        local_save_path = os.path.join(note_img_local_dir, new_filename)

        if os.path.exists(raw_img_path):
            try:
                shutil.copy2(raw_img_path, local_save_path)
                temp_tag = f"__IMG_TAG_{new_filename}__"
                img_mapping[local_save_path] = {
                    "new_filename": new_filename,
                    "temp_tag": temp_tag,
                    "original_path": raw_img_path,
                }
                print(f"âœ… å¤„ç†å›¾ç‰‡ï¼š{raw_img_path} â†’ {local_save_path}")
                return f"![{alt_text}]({temp_tag})"
            except Exception as e:
                print(f"âŒ å¤åˆ¶å›¾ç‰‡å¤±è´¥ï¼š{raw_img_path} â†’ {e}")
                process_success = False
                return match.group(0)
        else:
            print(f"âŒ å›¾ç‰‡ä¸å­˜åœ¨ï¼š{raw_img_path}")
            process_success = False
            return match.group(0)

    content = img_pattern.sub(replace_img_path, content)

    html_img_pattern = r'<img.*?src=["\'](.*?\.(png|jpg|jpeg|gif|webp))["\'].*?>'
    html_img_pattern = re.compile(html_img_pattern, re.IGNORECASE | re.DOTALL)

    def replace_html_img(match):
        nonlocal img_counter, process_success
        img_path = match.group(1).strip()
        img_ext = match.group(2).lower()

        if os.path.isabs(img_path):
            raw_img_path = img_path
        else:
            raw_img_path = os.path.join(md_dir, img_path)
        raw_img_path = os.path.abspath(raw_img_path)
        raw_img_path = raw_img_path.replace("/", "\\")

        new_filename = f"{md_basename}_{int(time.time())}_{img_counter}.{img_ext}"
        new_filename = replace_space_char(new_filename)
        img_counter += 1
        local_save_path = os.path.join(note_img_local_dir, new_filename)

        if os.path.exists(raw_img_path):
            try:
                shutil.copy2(raw_img_path, local_save_path)
                temp_tag = f"__IMG_TAG_{new_filename}__"
                img_mapping[local_save_path] = {
                    "new_filename": new_filename,
                    "temp_tag": temp_tag,
                    "original_path": raw_img_path,
                }
                print(f"âœ… å¤„ç†HTMLå›¾ç‰‡ï¼š{raw_img_path} â†’ {local_save_path}")
                return f'<img src="{temp_tag}" alt="image" title="image">'
            except Exception as e:
                print(f"âŒ å¤åˆ¶HTMLå›¾ç‰‡å¤±è´¥ï¼š{raw_img_path} â†’ {e}")
                process_success = False
                return match.group(0)
        else:
            print(f"âŒ HTMLå›¾ç‰‡ä¸å­˜åœ¨ï¼š{raw_img_path}")
            process_success = False
            return match.group(0)

    content = html_img_pattern.sub(replace_html_img, content)
    temp_content = content

    print(f"\nğŸ“Š å›¾ç‰‡å¤„ç†ç»Ÿè®¡ï¼šå…±å¤„ç† {len(img_mapping)} å¼ å›¾ç‰‡ï¼ˆåŸå§‹è·¯å¾„åˆ†å¸ƒä¸åŒï¼‰")
    return content, img_mapping, process_success


def clean_markdown_for_theme(raw_content: str, md_file_path: str) -> tuple[str, bool]:
    print_step("Markdownæ ¼å¼æ¸…æ´— + åŠ¨æ€å›¾ç‰‡è·¯å¾„å¤„ç†", 3)

    content = raw_content
    original_length = len(content)

    content, _, process_success = process_markdown_images(content, md_file_path)
    if not process_success:
        print("âŒ å›¾ç‰‡å¤„ç†å¤±è´¥")
        return content, False

    content = re.sub(r"<[^>]+>", "", content)
    print("âœ… ç§»é™¤å¤šä½™HTMLæ ‡ç­¾")

    def full_to_half(s):
        result = []
        for char in s:
            code = ord(char)
            if code == 12288:
                result.append(" ")
            elif 65281 <= code <= 65374:
                result.append(chr(code - 65248))
            else:
                result.append(char)
        return "".join(result)

    content = full_to_half(content)

    content = re.sub(r"^(#+)(\S)", r"\1 \2", content, flags=re.MULTILINE)
    content = re.sub(r"^(#+)\s+(.+?)\s+#+$", r"\1 \2", content, flags=re.MULTILINE)
    content = re.sub(r"^#{7,}", "######", content, flags=re.MULTILINE)
    content = re.sub(
        r'(#{1,6}\s+)(.+?)["&<>/\\:*\?|]+', r"\1\2", content, flags=re.MULTILINE
    )

    content = re.sub(r"\n{3,}", "\n\n", content)
    content = re.sub(r" {2,}", " ", content)
    content = re.sub(r"^\s+$", "", content, flags=re.MULTILINE)
    content = content.replace("\r\n", "\n").replace("\r", "\n")

    print(
        f"\nğŸ“Š æ ¼å¼æ¸…æ´—ç»Ÿè®¡ï¼šåŸå§‹ {original_length} å­—ç¬¦ â†’ æ¸…æ´—å {len(content)} å­—ç¬¦"
    )
    print("âœ… æ ¼å¼æ¸…æ´—å®Œæˆ")
    return content, True


# ========== FTPå‡½æ•°ï¼ˆå®Œå…¨ä¿ç•™ï¼‰ ==========
def ftp_init_connection() -> tuple[bool, FTP]:
    global ftp_conn
    try:
        ftp = FTP()
        ftp.set_pasv(FTP_PASSIVE)
        ftp.connect(FTP_HOST, FTP_PORT, timeout=FTP_TIMEOUT)
        ftp.login(FTP_USER, FTP_PWD)
        ftp.encoding = "utf-8"

        current_dir = ftp.pwd()
        print(f"âœ… FTPç™»å½•æˆåŠŸï¼å½“å‰æ ¹ç›®å½•ï¼š{current_dir}")

        ftp_conn = ftp
        return True, ftp
    except error_temp as e:
        print(f"âŒ FTPè¿æ¥è¶…æ—¶ï¼š{e}")
        return False, None
    except error_perm as e:
        print(f"âŒ FTPç™»å½•å¤±è´¥ï¼š{e}")
        return False, None
    except Exception as e:
        print(f"âŒ FTPåˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        return False, None


def ftp_verify_file_exists(remote_dir: str, filename: str) -> bool:
    if not ftp_conn:
        print(f"âŒ FTPæœªè¿æ¥ï¼Œæ— æ³•éªŒè¯æ–‡ä»¶ï¼š{remote_dir}{filename}")
        return False

    try:
        original_dir = ftp_conn.pwd()
        ftp_conn.cwd(remote_dir)
        file_list = ftp_conn.nlst()
        exists = filename in file_list
        ftp_conn.cwd(original_dir)

        if exists:
            print(f"âœ… FTPéªŒè¯æˆåŠŸï¼šæ–‡ä»¶ {remote_dir}{filename} å­˜åœ¨")
        else:
            print(f"âŒ FTPéªŒè¯å¤±è´¥ï¼šæ–‡ä»¶ {remote_dir}{filename} ä¸å­˜åœ¨")
        return exists
    except Exception as e:
        print(f"âš ï¸ FTPéªŒè¯å¼‚å¸¸ï¼š{e}")
        return False


def ftp_upload_file_with_verify(
    local_file_path: str, remote_dir: str, new_filename: str
) -> bool:
    if not ftp_conn:
        print(f"âŒ FTPæœªè¿æ¥ï¼Œè·³è¿‡ä¸Šä¼ ï¼š{local_file_path}")
        return False

    try:
        dirs = remote_dir.strip("/").split("/")
        current_dir = ""
        for dir_name in dirs:
            if dir_name:
                current_dir += f"/{dir_name}"
                try:
                    ftp_conn.cwd(current_dir)
                except error_perm:
                    ftp_conn.mkd(current_dir)
                    ftp_conn.cwd(current_dir)

        with open(local_file_path, "rb") as f:
            ftp_conn.storbinary(f"STOR {new_filename}", f, blocksize=8192)

        return ftp_verify_file_exists(remote_dir, new_filename)
    except Exception as e:
        print(f"âŒ FTPä¸Šä¼ å¤±è´¥ï¼š{local_file_path} â†’ {e}")
        return False


def ftp_batch_verify_files(remote_dir: str, img_mapping: dict) -> tuple[bool, list]:
    print(f"\nğŸ” æ‰¹é‡FTPéªŒè¯æ–‡ä»¶ï¼ˆç›®å½•ï¼š{remote_dir}ï¼‰...")
    success_count = 0
    fail_files = []
    total_files = len(img_mapping)

    for local_path, img_info in img_mapping.items():
        filename = img_info["new_filename"]
        if ftp_verify_file_exists(remote_dir, filename):
            success_count += 1
        else:
            fail_files.append(filename)

    print(f"ğŸ“Š FTPéªŒè¯ç»Ÿè®¡ï¼šæˆåŠŸ {success_count}/{total_files}ï¼Œå¤±è´¥ {len(fail_files)}")
    if fail_files:
        print(f"âŒ éªŒè¯å¤±è´¥çš„æ–‡ä»¶ï¼š{fail_files}")
    return success_count > 0 and success_count >= total_files * 0.9, fail_files


def ftp_close_connection():
    global ftp_conn
    if ftp_conn:
        try:
            ftp_conn.quit()
            print("âœ… FTPè¿æ¥å·²å…³é—­")
        except:
            ftp_conn.close()
        ftp_conn = None


# ========== 4. å‘å¸ƒæ–‡ç« å¹¶éªŒè¯ï¼ˆå®Œå…¨ä¿ç•™ï¼‰ ==========
def extract_article_id(
    session: requests.Session, publish_resp: requests.Response, article_title: str
) -> str:
    global article_id
    all_matched_ids = []
    title_escaped = re.escape(article_title)

    try:
        redirect_url = publish_resp.headers.get("Location", "")
        if redirect_url:
            id_match1 = re.search(r"/archives/(\d+)/?", redirect_url)
            id_match2 = re.search(r"cid=(\d+)", redirect_url)
            if id_match1:
                all_matched_ids.append(id_match1.group(1))
            if id_match2:
                all_matched_ids.append(id_match2.group(1))

        print(f"ğŸ” æå–æ–‡ç« IDï¼š{article_title}")
        manage_resp = session.get(TYPECHO_MANAGE_POSTS_URL, timeout=10)
        manage_html = decode_response(manage_resp)

        precise_pattern = (
            rf'<a href="[^"]*write-post\.php\?cid=(\d+)"[^>]*>{title_escaped}</a>'
        )
        precise_matches = re.findall(precise_pattern, manage_html, re.IGNORECASE)
        if precise_matches:
            all_matched_ids.extend(precise_matches)

        cid_matches = re.findall(r"write-post\.php\?cid=(\d+)", manage_html)
        archive_matches = re.findall(r"/index\.php/archives/(\d+)/", manage_html)
        all_matched_ids.extend(cid_matches)
        all_matched_ids.extend(archive_matches)

        if all_matched_ids:
            unique_ids = list(
                set([id_str for id_str in all_matched_ids if id_str.isdigit()])
            )
            if unique_ids:
                article_id = str(max([int(id_str) for id_str in unique_ids]))
                print(f"âœ… æå–æ–‡ç« IDï¼š{article_id}")
                return article_id

        print("âŒ æœªèƒ½æå–æ–‡ç« ID")
        return None
    except Exception as e:
        print(f"âŒ æå–IDå¤±è´¥ï¼š{e}")
        return None


def verify_article_published(session: requests.Session, title: str) -> bool:
    print(f"\nğŸ” éªŒè¯æ–‡ç« å‘å¸ƒçŠ¶æ€ï¼š{title}")
    title_escaped = re.escape(title)

    for page in range(1, 5):
        manage_url = f"{TYPECHO_MANAGE_POSTS_URL}?page={page}"
        try:
            resp = session.get(manage_url, timeout=10)
            if resp.status_code == 200 and re.search(
                rf"<a[^>]*>{title_escaped}</a>", decode_response(resp)
            ):
                print(f"âœ… æ–‡ç« å‘å¸ƒéªŒè¯é€šè¿‡ï¼")
                return True
        except Exception as e:
            print(f"âš ï¸ æ£€æŸ¥ç¬¬{page}é¡µå¤±è´¥ï¼š{e}")

    print(f"âŒ æ–‡ç« å‘å¸ƒéªŒè¯å¤±è´¥")
    return False


def publish_article(
    session: requests.Session, title: str, cleaned_content: str, category_ids: list[int]
) -> bool:
    print_step("å‘å¸ƒæ–‡ç« å¹¶éªŒè¯", 4)
    global article_id
    article_id = None

    try:
        update_referer_headers(session, TYPECHO_ADMIN_URL)
        write_resp = session.get(TYPECHO_WRITE_URL, timeout=10)
        if write_resp.status_code != 200:
            raise Exception(f"å‘å¸ƒé¡µè®¿é—®å¤±è´¥ï¼š{write_resp.status_code}")
        write_html = decode_response(write_resp)

        publish_token = re.search(r"_=([0-9a-f]{32})", write_html)
        publish_token = (
            publish_token.group(1)
            if publish_token
            else "4bc337a9bb2079e48605260b98bcc6d8"
        )
        csrf_token = re.search(r'name="__typecho_csrf_token" value="(.*?)"', write_html)
        csrf_token = csrf_token.group(1) if csrf_token else ""

        publish_api = (
            f"{SITE_HOME}/index.php/action/contents-post-edit?_={publish_token}"
        )
        publish_data = {
            "title": title,
            "text": cleaned_content,
            "markdown": "1",
            "visibility": "publish",
            "do": "publish",
            "timezone": TIMEZONE,
            "__typecho_csrf_token": csrf_token,
            "submit": "å‘å¸ƒ",
        }
        for cid in category_ids:
            publish_data[f"category[]"] = str(cid)

        publish_headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Origin": SITE_HOME,
            "Referer": TYPECHO_WRITE_URL,
            **CHROME_HEADERS,
        }
        publish_resp = session.post(
            publish_api,
            data=publish_data,
            headers=publish_headers,
            allow_redirects=False,
            timeout=30,
            verify=False,
        )
        print(f"ğŸ“ˆ å‘å¸ƒå“åº”ç ï¼š{publish_resp.status_code}")

        article_id = extract_article_id(session, publish_resp, title)
        if not article_id:
            print("âŒ æå–æ–‡ç« IDå¤±è´¥ï¼Œå‘å¸ƒéªŒè¯æ— æ³•è¿›è¡Œ")
            return False

        if verify_article_published(session, title):
            return True
        else:
            rollback_article(session, article_id)
            return False
    except Exception as e:
        print(f"âŒ å‘å¸ƒæ–‡ç« å¤±è´¥ï¼š{e}")
        import traceback

        traceback.print_exc()
        if article_id:
            rollback_article(session, article_id)
        return False


# ========== 5. ä¸Šä¼ å›¾ç‰‡å¹¶éªŒè¯ï¼ˆå®Œå…¨ä¿ç•™ï¼‰ ==========
def update_article_img_links(
    session: requests.Session, article_id: str, article_title: str
) -> bool:
    global temp_content, img_mapping
    if not img_mapping or not article_id:
        print("â„¹ï¸ æ— å›¾ç‰‡éœ€è¦æ›´æ–°é“¾æ¥")
        return True

    try:
        updated_content = temp_content
        remote_dir = f"/{article_id}/"
        for local_path, img_info in img_mapping.items():
            temp_tag = img_info["temp_tag"]
            new_filename = img_info["new_filename"]
            final_url = f"{IMG_SERVER_URL}{article_id}/{new_filename}"
            updated_content = updated_content.replace(temp_tag, final_url)

        edit_url = f"{SITE_HOME}/admin/write-post.php?cid={article_id}"
        update_referer_headers(session, TYPECHO_MANAGE_POSTS_URL)
        edit_resp = session.get(edit_url, timeout=10)
        edit_html = decode_response(edit_resp)

        if article_title not in edit_html:
            print(f"âš ï¸ ç¼–è¾‘é¡µæ— æ ‡é¢˜ï¼š{article_title}")
            return False

        publish_token = re.search(r"_=([0-9a-f]{32})", edit_html)
        publish_token = (
            publish_token.group(1)
            if publish_token
            else "4bc337a9bb2079e48605260b98bcc6d8"
        )
        csrf_token = re.search(r'name="__typecho_csrf_token" value="(.*?)"', edit_html)
        csrf_token = csrf_token.group(1) if csrf_token else ""

        update_api = (
            f"{SITE_HOME}/index.php/action/contents-post-edit?_={publish_token}"
        )
        update_data = {
            "cid": article_id,
            "title": article_title,
            "text": updated_content,
            "markdown": "1",
            "visibility": "publish",
            "do": "publish",
            "timezone": TIMEZONE,
            "__typecho_csrf_token": csrf_token,
            "submit": "ä¿å­˜",
        }
        for cid in category_map.keys():
            update_data[f"category[]"] = str(cid)

        update_headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Origin": SITE_HOME,
            "Referer": edit_url,
            **CHROME_HEADERS,
        }
        update_resp = session.post(
            update_api,
            data=update_data,
            headers=update_headers,
            allow_redirects=False,
            timeout=30,
            verify=False,
        )

        if update_resp.status_code in [200, 302]:
            print(f"âœ… æ–‡ç« é“¾æ¥æ›´æ–°æˆåŠŸï¼ˆçŠ¶æ€ç ï¼š{update_resp.status_code}ï¼‰")
            return True
        else:
            print(f"âŒ é“¾æ¥æ›´æ–°å¤±è´¥ï¼ˆçŠ¶æ€ç ï¼š{update_resp.status_code}ï¼‰")
            return False
    except Exception as e:
        print(f"âŒ æ›´æ–°é“¾æ¥å¤±è´¥ï¼š{e}")
        import traceback

        traceback.print_exc()
        return False


def upload_and_verify_images(
    session: requests.Session, article_id: str, article_title: str
) -> bool:
    print_step("ä¸Šä¼ å›¾ç‰‡å¹¶éªŒè¯", 5)
    if not img_mapping or not article_id:
        print("â„¹ï¸ æ— å›¾ç‰‡éœ€è¦ä¸Šä¼ ")
        return True

    ftp_success, _ = ftp_init_connection()
    if not ftp_success:
        print("âŒ FTPè¿æ¥å¤±è´¥ï¼Œå›¾ç‰‡ä¸Šä¼ ç»ˆæ­¢")
        rollback_article(session, article_id)
        return False

    try:
        remote_dir = f"/{article_id}/"
        print(f"\nğŸ“¤ å¼€å§‹ä¸Šä¼ å›¾ç‰‡åˆ° /{article_id}/ ç›®å½•ï¼ˆå…±{len(img_mapping)}å¼ ï¼‰...")

        all_upload_success = True
        for local_path, img_info in img_mapping.items():
            if not ftp_upload_file_with_verify(
                local_path, remote_dir, img_info["new_filename"]
            ):
                all_upload_success = False

        batch_verify_success, fail_files = ftp_batch_verify_files(
            remote_dir, img_mapping
        )
        if not batch_verify_success:
            print(f"âŒ å›¾ç‰‡æ‰¹é‡éªŒè¯å¤±è´¥ï¼š{fail_files}")
            rollback_ftp_files(remote_dir)
            rollback_article(session, article_id)
            return False

        if not update_article_img_links(session, article_id, article_title):
            print(f"âš ï¸ é“¾æ¥æ›´æ–°å¤±è´¥ï¼Œå¯æ‰‹åŠ¨ç¼–è¾‘ä¿®æ­£")

        print("âœ… å›¾ç‰‡ä¸Šä¼ å¹¶éªŒè¯æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ å›¾ç‰‡ä¸Šä¼ å¤±è´¥ï¼š{e}")
        import traceback

        traceback.print_exc()
        if article_id:
            rollback_ftp_files(f"/{article_id}/")
            rollback_article(session, article_id)
        return False
    finally:
        ftp_close_connection()


# ========== 6. å…³é—­å¹¶é‡Šæ”¾Sessionï¼ˆå®Œå…¨ä¿ç•™ï¼‰ ==========
def release_resources(session: requests.Session):
    print_step("å…³é—­å¹¶é‡Šæ”¾Session", 6)
    try:
        session.close()
        print("âœ… Sessionå·²é‡Šæ”¾")
    except Exception as e:
        print(f"âš ï¸ é‡Šæ”¾Sessionå¼‚å¸¸ï¼š{e}")
    ftp_close_connection()


# ========== æ‰¹é‡æ–‡ä»¶å¤„ç†ï¼ˆå®Œå…¨ä¿ç•™ï¼‰ ==========
def select_files_to_publish(folder_path: str) -> list[str]:
    md_files = []
    for file in os.listdir(folder_path):
        if file.lower().endswith(".md"):
            md_files.append(os.path.join(folder_path, file))
    md_files.sort()

    if not md_files:
        print(f"âŒ æ–‡ä»¶å¤¹ {folder_path} ä¸‹æ— MDæ–‡ä»¶")
        sys.exit(1)

    print("\n" + "=" * 60)
    print("ğŸ“‹ æ–‡ä»¶å¤¹ä¸‹çš„MDæ–‡ä»¶åˆ—è¡¨ï¼ˆæŒ‰åç§°æ’åºï¼‰")
    print("=" * 60)
    for idx, file in enumerate(md_files, 1):
        file_name = os.path.basename(file)
        print(f"   {idx}. {file_name}")
    print("=" * 60)
    print("æç¤ºï¼šè¾“å…¥è¦å‘å¸ƒçš„æ–‡ä»¶ç¼–å·ï¼Œæ”¯æŒæ ¼å¼ï¼š")
    print("   å•ä¸ªé€‰æ‹©ï¼š1 3 5")
    print("   èŒƒå›´é€‰æ‹©ï¼š1-3")
    print("   å…¨éƒ¨é€‰æ‹©ï¼šall")

    while True:
        user_input = input("è¯·è¾“å…¥é€‰æ‹©ï¼ˆç›´æ¥å›è½¦é€€å‡ºï¼‰ï¼š").strip()
        if not user_input:
            sys.exit(0)
        if user_input.lower() == "all":
            print(f"âœ… é€‰æ‹©å…¨éƒ¨ {len(md_files)} ä¸ªæ–‡ä»¶")
            return md_files

        selected_nums = parse_user_selection(user_input, len(md_files))
        if selected_nums:
            selected_files = [md_files[num - 1] for num in selected_nums]
            print(f"\nâœ… å·²é€‰æ‹© {len(selected_files)} ä¸ªæ–‡ä»¶ï¼š")
            for file in selected_files:
                print(f"   - {os.path.basename(file)}")
            return selected_files
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥ï¼")


def process_selected_files(session: requests.Session, selected_files: list[str]):
    category_ids = select_categories()

    batch_stats["total"] = len(selected_files)
    print(f"\nâœ… å¼€å§‹å‘å¸ƒ {len(selected_files)} ä¸ªæ–‡ä»¶...")
    for idx, file in enumerate(selected_files, 1):
        print(f"\n{'='*100}")
        print(f"ğŸ“„ å¤„ç†è¿›åº¦ï¼š{idx}/{len(selected_files)}")
        print(f"{'='*100}")
        if publish_single_file(session, file, category_ids):
            batch_stats["success"] += 1
        else:
            batch_stats["failed"] += 1
            batch_stats["failed_files"].append(file)
        human_delay(BATCH_DELAY, BATCH_DELAY)

    print("\n" + "=" * 100)
    print("ğŸ“Š æ‰¹é‡å‘å¸ƒç»“æœæ±‡æ€»")
    print("=" * 100)
    print(f"ğŸ“ æ€»æ–‡ä»¶æ•°ï¼š{batch_stats['total']}")
    print(f"âœ… æˆåŠŸæ•°ï¼š{batch_stats['success']}")
    print(f"âŒ å¤±è´¥æ•°ï¼š{batch_stats['failed']}")
    if batch_stats["failed_files"]:
        print(f"\nâŒ å¤±è´¥æ–‡ä»¶åˆ—è¡¨ï¼š")
        for file in batch_stats["failed_files"]:
            print(f"   - {os.path.basename(file)}")


def publish_single_file(
    session: requests.Session, md_file_path: str, category_ids: list[int]
) -> bool:
    reset_global_vars()
    try:
        with open(md_file_path, "r", encoding="utf-8") as f:
            raw_content = f.read()

        title = os.path.splitext(os.path.basename(md_file_path))[0]
        title = replace_space_char(title)
        print(f"âœ… æ–‡ç« æ ‡é¢˜ï¼š{title}")

        cleaned_content, process_success = clean_markdown_for_theme(
            raw_content, md_file_path
        )
        if not process_success:
            print(f"âŒ æ­¥éª¤3å¤±è´¥ï¼šæ ¼å¼æ¸…æ´—/å›¾ç‰‡å¤„ç†å¤±è´¥")
            return False

        if not publish_article(session, title, cleaned_content, category_ids):
            print(f"âŒ æ­¥éª¤4å¤±è´¥ï¼šæ–‡ç« å‘å¸ƒ/éªŒè¯å¤±è´¥")
            clean_local_img_dir(note_img_local_dir)
            return False

        if not upload_and_verify_images(session, article_id, title):
            print(f"âŒ æ­¥éª¤5å¤±è´¥ï¼šå›¾ç‰‡ä¸Šä¼ /éªŒè¯å¤±è´¥")
            clean_local_img_dir(note_img_local_dir)
            return False

        clean_local_img_dir(note_img_local_dir)
        print(f"\nâœ… æ–‡ä»¶ {md_file_path} å…¨éƒ¨æ­¥éª¤æ‰§è¡ŒæˆåŠŸï¼")
        return True

    except Exception as e:
        print(f"âŒ æ–‡ä»¶å¤„ç†å¼‚å¸¸ï¼š{e}")
        import traceback

        traceback.print_exc()
        if article_id:
            rollback_article(session, article_id)
            rollback_ftp_files(f"/{article_id}/")
        clean_local_img_dir(note_img_local_dir)
        return False


# ========== ä¸»å‡½æ•°ï¼ˆå®Œå…¨ä¿ç•™ï¼‰ ==========
def main():
    print("=" * 80)
    print("ğŸ¯ Typecho æ‰¹é‡å‘å¸ƒå·¥å…·ï¼ˆYAMLé…ç½®ç‰ˆï¼‰")
    print("âœ… ç‰¹æ€§ï¼šYAMLå¤šå±‚æ’å€¼ + åŠ¨æ€å›¾ç‰‡è·¯å¾„è§£æ + ä»…æ‰¹é‡é€‰æ‹©æ–‡ä»¶")
    print("=" * 80)

    if len(sys.argv) != 2:
        print("âŒ ä½¿ç”¨æ–¹æ³•ï¼špython script.py <MDæ–‡ä»¶å¤¹è·¯å¾„>")
        print("   ç¤ºä¾‹ï¼špython script.py 'D:/ç¬”è®°/å¾…å‘å¸ƒ'")
        sys.exit(1)

    target_path = sys.argv[1].strip("'\"")

    if not os.path.isdir(target_path):
        print(f"âŒ æ— æ•ˆè·¯å¾„ï¼š{target_path}ï¼ˆå¿…é¡»æ˜¯æ–‡ä»¶å¤¹è·¯å¾„ï¼‰")
        sys.exit(1)

    login_success, session = simulate_browser_login()
    if not login_success:
        print("\nâŒ ç™»å½•éªŒè¯å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
        sys.exit(1)

    crawl_categories(session)

    selected_files = select_files_to_publish(target_path)
    if not selected_files:
        release_resources(session)
        sys.exit(0)

    process_selected_files(session, selected_files)

    release_resources(session)
    print("\nğŸ”Œ æ‰€æœ‰è¿æ¥å·²é‡Šæ”¾ï¼Œæ‰¹é‡å¤„ç†å®Œæˆï¼")


if __name__ == "__main__":
    main()
